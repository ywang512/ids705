{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Supervised Learning\n",
    "\n",
    "## *YOUR FULL NAME HERE*\n",
    "Netid:  *Your netid here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "### [40 points] From theory to practice: classification through logistic regression\n",
    "\n",
    "#### Introduction\n",
    "For this problem you will derive, implement through gradient descent, and test the performance of a logistic regression classifier for a binary classification problem.\n",
    "\n",
    "In this case, we'll assume our logistic regression problem will be applied to a two dimensional feature space. Our logistic regression model is:\n",
    "\n",
    "$$f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^T \\mathbf{x}_i)$$\n",
    "\n",
    "where the sigmoid function is defined as $\\sigma(x) = \\frac{e^x}{1+e^{x}}= \\frac{1}{1+e^{-x}}$. Also, since this is a two-dimensional problem, we define $\\mathbf{w}^T \\mathbf{x}_i = w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}$ and here, $x_{i,0} \\triangleq 1$\n",
    "\n",
    "As in class, we will interpret the response of the logistic regression classifier to be the likelihood of the data given the model. For one sample, $(y_i, \\mathbf{x_i})$, this is given as:\n",
    "\n",
    "$$P(Y=y_i|X=x_i) = f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^T \\mathbf{x}_i)$$\n",
    "\n",
    "#### Find the cost function that we can use to choose the model parameters, $\\mathbf{w}$, that best fit the training data.\n",
    "\n",
    "**(a)** What is the likelihood function of the data that we will wish to maximize?\n",
    "\n",
    "**(b)** Since a logarithm is a monotonic function, maximizing the $f(x)$ is equivalent to maximizing $\\ln [f(x)]$. Express part (a) as a cost function of the model parameters, $C(\\mathbf{w})$, that is the negative of the logarithm of (a).\n",
    "\n",
    "**(c)** Calculate the gradient of the cost function with respect to the model parameters $\\nabla_{\\mathbf{w}}C(\\mathbf{w})$. Express this in terms of the partial derivatives of the cost function with respect to each of the parameters, e.g. $\\nabla_{\\mathbf{w}}C(\\mathbf{w}) = \\left[\\frac{\\partial C}{\\partial w_0}, \\frac{\\partial C}{\\partial w_1}, \\frac{\\partial C}{\\partial w_2}\\right]$.\n",
    "\n",
    "**(d)** Write out the gradient descent update equation, assuming $\\eta$ represents the learning rate.\n",
    "\n",
    "#### Prepare and plot your data\n",
    "\n",
    "**(e)** Load the data and scatter plot the data by class. In the data folder in the same directory of this notebook, you'll find the data in `A3_Q1_data.csv`. This file contains the binary class labels, $y$, and the features $x_1$ and $x_2$.  Comment on the data: do the data appear separable? Why might logistic regression be a good choice for these data or not?\n",
    "\n",
    "**(f)** Do the data require any preprocessing due to missing values, scale differences, etc? If so, how did you remediate this?\n",
    "\n",
    "#### Implement gradient descent and your logistic regression algorithm\n",
    "\n",
    "**(g)** Create a function or class to implement your logistic regression. It should take as inputs the model parameters, $\\mathbf{w}=\\left[w_0,w_1,w_2\\right]$, and output the class confidence probabilities, $P(Y=y_i|X=x_i)$.\n",
    "\n",
    "**(h)** Create a function that computes the cost function $C(\\mathbf{w})$ for a given dataset and corresponding class labels.\n",
    "\n",
    "**(i)** Create a function or class to run gradient descent on your training data. We'll refer to this as \"batch\" gradient descent since it takes into account the gradient based on all our data at each iteration (or \"epoch\") of the algorithm. Divide your data into a training and testing set where the test set accounts for 30 percent of the data and the training set the remaining 70 percent. In doing this we'll need to make some assumptions / experiment with the following:\n",
    "1. The initialization of the algorithm - what should you initialize the model parameters to? For this, randomly initialize the weights to a different values between 0 and 1.\n",
    "2. The learning rate - how slow/fast should the algorithm proceed in the direction opposite the gradient? This you will experiment with.\n",
    "3. Stopping criteria - when should the algorithm be finished searching for the optimum? Set this to be when the cost function changes by no more than $10^{-6}$ between iterations. Since we have a weight vector, you can compute this by seeing if the L2 norm of the weight vector changes by no more than $10^{-6}$ between iterations.\n",
    "\n",
    "**(j)** At each step in the gradient descent algorithm it will produce updated parameter estimates. For each set of estimates, calculate the cost function for both the training and the test data. \n",
    "\n",
    "**(k)** Show this process for different learning rates by plotting the resulting cost as a function of iteration (or \"epoch\"). What is the impact that each parameter has on the process and the results? What choices did you make in your chosen approach and why? Use the parameter you choose here for the learning rate for the remainder of this question.\n",
    "\n",
    "#### Test your model performance through cross validation\n",
    "\n",
    "**(l)** Test the performance of your trained classifier using K-folds cross validation (while this can be done manually, the scikit-learn package [StratifiedKFolds](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) may be helpful). Produce Receiver Operating Characteristic Curves (ROC curves) of your cross validated performance. \n",
    "\n",
    "**(m)** Why do we use cross validation?\n",
    "\n",
    "**(n)** Make two plots - one of your training data, and one for your test data - with the data scatterplotted and the decision boundary for your classifier. Comment on your decision boundary. Could it be improved?\n",
    "\n",
    "**(o)** Compare your trained model to random guessing. Show the ROC curve for your model and plot the chance diagonal. What area under the curve (AUC) does your model achieve? How does your model compare in terms of performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "### [20 points] Digits classification\n",
    "\n",
    "**(a)** Construct your dataset from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of handwritten digits, which has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "Your goal is to determine whether or not an example is a 3, therefore your binary classifier will seek to estimate $y=1$ if the digit is a 3, and $y=0$ otherwise. Create your dataset by transforming your labels into a binary format. \n",
    "\n",
    "**(b)** Plot 10 examples of each class 0 and 1, from the training dataset.\n",
    "\n",
    "**(c)** How many examples are present in each class? Are the classes balanced? What issues might this cause?\n",
    "\n",
    "**(d)** Using cross-validation, train and test a classifier. Compare your performance against (1) a classifier that randomly guesses the class, and (2) a classifier that guesses that all examples are NOT 3's. Plot corresponding ROC curves and precision-recall curves. Describe the algorithm's performance and explain any discrepancies you find.\n",
    "\n",
    "**(f)** Using a logistic regression classifier (a linear classifier), apply lasso regularization and retrain the model and evaluate its performance over a range of values on the regularization coefficient. You can implement this using the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) module (DO NOT use your function from question 1) and activating the 'l1' penalty; the parameter $C$ is the inverse of the regularization strength. As you vary the regularization coefficient, plot (1) the number of model parameters that are estimated to be nonzero; (2) the logistic regression cost function, which you created a function to evaluate in the Question 1; (3) $F_1$-score, and (4) area under the curve (AUC). Describe the implications of your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "### [40 points] Supervised learning exploration\n",
    "\n",
    "For this exercise, you will construct and implement a supervised learning problem solution/experiment. Describe your process and answer these questions clearly and thoroughly. Part of the grade in this assignment is devoted to the quality and professionalism of your work.\n",
    "\n",
    "**(a)** Identify a question or problem that's of interest to you and that could be addressed using classification or regression. Explain why it's interesting and what you'd like to accomplish. This should exhibit creativity, and you are not allowed to use the Iris dataset, the Kaggle Titanic dataset, or the Kaggle chocolate dataset.\n",
    "\n",
    "**(b)** Download the data and plot the data to describe it.\n",
    "\n",
    "**(c)** Formulate your supervised learning question: (a) What is your target variable (what are you trying to predict) and what predictors do you have available? \n",
    "v Does your dataset require any preprocessing: is it clean (no missing values or erroneous data) and normalized (are each of the predictors of the same magnitude)? \n",
    "\n",
    "**(d)** What supervised learning technique will you use and why? \n",
    "\n",
    "**(e)** How will you evaluate performance and know whether you succeeded (e.g. ROC curves for binary classification, mean square error or $R^2$ for regression)?\n",
    "\n",
    "**(f)** Divide your dataset into training and testing datasets OR implement cross validation. Explain your approach and why you adopted it.\n",
    "\n",
    "**(g)** Run your analysis and show your performance. Include plots of your data and of performance.\n",
    "\n",
    "**(h)** Describe how your system performed, where your supervised learning algorithm performed well, and where it did not, and how you could improve it.\n",
    "\n",
    "**(i)** Write a brief summary / elevator pitch for this work that you would put on LinkedIn to describe this project to future employers. This should focus on the high level impact and importance and overall takeaways and not on the nitty-gritty details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
